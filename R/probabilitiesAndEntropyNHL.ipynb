{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We estimate the joint probability function of the dataset and estimate the entropy of various subsets. Additionally we estimate the conditional probability of game success by looking at the various statistics at our disposal. For our purpose we use the package ks (https://cran.r-project.org/web/packages/ks/index.html), which contains density function estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: KernSmooth\n",
      "KernSmooth 2.23 loaded\n",
      "Copyright M. P. Wand 1997-2009\n",
      "Loading required package: misc3d\n",
      "Loading required package: mvtnorm\n",
      "Loading required package: rgl\n"
     ]
    }
   ],
   "source": [
    "library(ks)\n",
    "DATACAPTIONVEC <- c(\"ID\",\"SEASON\",\"DATE\",\"TEAM1\",\"TEAM2\",\"WON\",\"SCORE\",\"SHOTS\",\"FACEOFF\",\"TAKEAWAY\",\"GIVEAWAY\",\"PIM\",\"HITS\",\"PPG\",\"ATTENDANCE\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the first step we read the input data as generated by the python class dataMiner.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nhlDataSum=data.frame()\n",
    "nhlDataDelta=data.frame()\n",
    "SeasonVector=c(2010)#,2011,2012,2014,2015,2016)\n",
    "NumberOfSeasons=length(SeasonVector)\n",
    "for(season in SeasonVector)\n",
    "{\n",
    "  tableName=paste(\"../dataSetsNHL/dataFileNhl_\",season,\"_regular_sum.dat\",sep=\"\")\n",
    "  nhlDataS=read.table(tableName)\n",
    "  colnames(nhlDataS) <- DATACAPTIONVEC\n",
    "  nhlDataSum<-rbind(nhlDataSum,nhlDataS)\n",
    "  tableName=paste(\"../dataSetsNHL/dataFileNhl_\",season,\"_regular_delta.dat\",sep=\"\")\n",
    "  nhlDataS=read.table(tableName)\n",
    "  colnames(nhlDataS) <- DATACAPTIONVEC\n",
    "  nhlDataDelta<-rbind(nhlDataDelta,nhlDataS)\n",
    "}\n",
    "colnames(nhlDataSum) <- DATACAPTIONVEC\n",
    "colnames(nhlDataDelta) <- DATACAPTIONVEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data consists of data with postfix Delta and Sum, with Sum data defined as $X_{\\Delta}=X_{h}-X_{a}$ with $X_{h,a} \\in X \\in \\mathbb{D}^{d}$ and $X_{\\Sigma}=X_{h}+X_{a}$ with $X_{h,a} \\in \\mathbb{D}^{d}$ ($\\mathbb{D}$ a mixture of binary and natural numbers), d dimensional home and away team statistics vectors. In our case every datasample has 16 entries,with 10 numerical attributes per dataset, giving d=10. The first 6 attributes of each dataset like season, game id and teams who played contain supplementery information and are important only in the preprocessing process. We may write our dataset as \"set\" $X_{\\Delta,Sum} = F ( x_{won},x_{score},x_{shots},x_{faceoff},x_{takeaway},x_{giveaway},x_{pim},x_{hits},x_{ppg},x_{attendance} )$ with F a function of the datasets for home and away teams, as described above. $x_{won}$ takes binary values $x_{won} \\in [0,1]$, 0 for loss and 1 for game won. The other attributes take integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datavectors $X_{Sum},X_{\\Delta}$ are now combined to give individual teams statistics. For home games we can simply add the two $X_{team,h}={1 \\over 2} \\{ X_{Sum} + X_{\\Delta} \\}$, for the away team stats we have to invert $X_{\\Delta}$: $X_{team,a}={1 \\over 2} \\{ X_{Sum} - X_{\\Delta} \\}$, and finally add the two: $X_{team}=X_{team,h}+X_{team,a}$ to obtain the overall game stats. We are doing this for all the teams contained in the statistics, which are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"we have the following teams\"\n",
      " [1] BOS       TOR       COL       CGY       CAR       PIT       CHI      \n",
      " [8] DET       BUF       NJ        NYI       NYR       WSH       ATLANTA  \n",
      "[15] FLA       CBJ       DAL       STL       EDM       ANA       LA       \n",
      "[22] VAN       PHI       MIN       TB        NSH       SJ        PHO      \n",
      "[29] CANADIENS OTT      \n",
      "30 Levels: ANA ATLANTA BOS BUF CANADIENS CAR CBJ CGY CHI COL DAL DET ... WSH\n"
     ]
    }
   ],
   "source": [
    "teams <- nhlDataDelta$TEAM1\n",
    "LISTOFTEAMS=unique(teams)\n",
    "print(\"we have the following teams\")\n",
    "print(LISTOFTEAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getTeamGameStatistics <- function(thisTeam)\n",
    "    {\n",
    "    matchS<-subset(nhlDataSum,nhlDataSum$TEAM1==thisTeam | nhlDataSum$TEAM2==thisTeam)\n",
    "    matchH<-subset(nhlDataDelta,nhlDataDelta$TEAM1==thisTeam)\n",
    "    matchA<-subset(nhlDataDelta,nhlDataDelta$TEAM2==thisTeam)\n",
    "    \n",
    "    #invert away data\n",
    "    matchA$SCORE=-matchA$SCORE\n",
    "    matchA$SHOTS=-matchA$SHOTS\n",
    "    matchA$FACEOFF=-matchA$FACEOFF\n",
    "    matchA$TAKEAWAY=-matchA$TAKEAWAY\n",
    "    matchA$GIVEAWAY=-matchA$GIVEAWAY\n",
    "    matchA$PIM=-matchA$PIM\n",
    "    matchA$HITS=-matchA$HITS\n",
    "    matchA$PPG=-matchA$PPG\n",
    "\n",
    "    #add delta data\n",
    "    matchD<-rbind(matchH,matchA)\n",
    "    #now order for data then season\n",
    "    tmp<-matchD[order(matchD$DATE),]\n",
    "    matchDOrdered<-tmp[order(tmp$SEASON),]\n",
    "\n",
    "    #compute the NYI values by combining delta and summed data\n",
    "    teamData=0.5*(matchDOrdered[,sapply(matchDOrdered,is.numeric)]+matchS[,sapply(matchS,is.numeric)])\n",
    "    return(teamData)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Have retrieved a data list of size 30 for  30 teams\"\n"
     ]
    }
   ],
   "source": [
    "teamsDataList <- list()\n",
    "for(i in 1:length(LISTOFTEAMS))\n",
    "{\n",
    "team=LISTOFTEAMS[i]\n",
    "teamsDataList[[i]] <- getTeamGameStatistics(team)\n",
    "}\n",
    "print(paste(\"Have retrieved a data list of size\",length(teamsDataList),\"for \",length(LISTOFTEAMS),\"teams\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step we coarse grain the data. One reason for this is to make the computational feat of multidimensional probability function estimation more feasible. The second is that the phase space volume is so vast and data scattered so sparsly, that an agglomeration into more coarse categories may not do harm in reducing the information content. The coarse graining is determined by one parameter only, the number of parts in which every data column is subdivided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coarseGraining <- function(dataMatrix,parts)\n",
    "{\n",
    "  #coarse graining of input data.matrix\n",
    "  grainedMatrix = matrix(nrow=nrow(dataMatrix),ncol=ncol(dataMatrix))\n",
    "  centroidMatrix = matrix(nrow=parts,ncol=ncol(dataMatrix))\n",
    "  for(j in 1:ncol(dataMatrix))\n",
    "  {\n",
    "    colVals=dataMatrix[,j]\n",
    "    coarsedGrainedColData=c()\n",
    "    maxV=max(colVals)\n",
    "    minV=min(colVals)\n",
    "    delta=(maxV-minV)/parts\n",
    "    centroidV=minV+0:(parts-1)*delta+delta/2\n",
    "    for(jj in 1:length(colVals))\n",
    "      {\n",
    "        #get the index of nearest element of column Values in centroidVector\n",
    "      actualVal<-colVals[jj]\n",
    "      coarsedGrainedColData[jj]=centroidV[which.min(abs(centroidV-colVals[jj]))]\n",
    "    }\n",
    "    #now add data to matrix\n",
    "    grainedMatrix[,j] <- coarsedGrainedColData\n",
    "    centroidMatrix[,j] <- centroidV\n",
    "  }#next matrix column\n",
    "  \n",
    "    #return the coarse grained data matrix and centroid vectors\n",
    "  return(list(\"coarseGrainedMatrix\"=grainedMatrix,\"centroidVectors\"=centroidMatrix))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"We analyze 1223 games characterized by 7 attributes.\"\n"
     ]
    }
   ],
   "source": [
    "nhlDataMatrix=as.matrix(cbind(nhlDataDelta$SHOTS,nhlDataDelta$FACEOFF,nhlDataDelta$HITS,nhlDataDelta$TAKEAWAY,nhlDataDelta$GIVEAWAY,nhlDataDelta$PIM,nhlDataDelta$PPG))\n",
    "print(paste(\"We analyze\",nrow(matchNYIMatrix),\"games characterized by\",ncol(matchNYIMatrix),\"attributes.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "returnVal=coarseGraining(nhlDataMatrix,4)\n",
    "McoarseGrained=returnVal$coarseGrainedMatrix\n",
    "centroidVectors=returnVal$centroidVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  fetchDataColumnsAsMatrix <- function(dataFrame)\n",
    "  {\n",
    "    #input dataframe\n",
    "    #output matrix of certain columns of dataframe, fixed in function\n",
    "    \n",
    "    return(as.matrix(cbind(dataFrame$SHOTS,dataFrame$FACEOFF,dataFrame$TAKEAWAY,dataFrame$PIM,dataFrame$PPG)))\n",
    "    \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Now we write a function to coarse grain data according to a precomputed coarse grain matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coarseGrainingGrainMatrix <- function(matrixToGrain,centroidVectors)\n",
    "{\n",
    "  #categorize input data in matrix grainMatrix according to centroids set in centroidVectors\n",
    "  #return matrix with values from centroidVectors\n",
    "  if(is.vector(matrixToGrain))\n",
    "  {\n",
    "    matrixToGrain <- as.matrix(matrixToGrain)\n",
    "    if(ncol(matrixToGrain)<nrow(matrixToGrain))\n",
    "      {\n",
    "      matrixToGrain <- t(matrixToGrain)\n",
    "      }\n",
    "    #grainedMatrix=as.matrix(matrixToGrain)\n",
    "  #  grainedMatrix = matrix(nrow=1,ncol=length(matrixToGrain))\n",
    "  }\n",
    "  #else\n",
    "  #{\n",
    "  grainedMatrix = matrix(nrow=nrow(matrixToGrain),ncol=ncol(matrixToGrain))\n",
    "  #}\n",
    "  for(j in 1:ncol(matrixToGrain))\n",
    "  { \n",
    "    centroidV=centroidVectors[,j]\n",
    "    colVals=matrixToGrain[,j]\n",
    "    coarsedGrainedColData=c()\n",
    "    for(jj in 1:length(colVals))\n",
    "    {\n",
    "      #get the index of nearest element of column Values in centroidVector\n",
    "      actualVal<-colVals[jj]\n",
    "      coarsedGrainedColData[jj] <- centroidV[which.min(abs(centroidV-actualVal))]\n",
    "    } \n",
    "    #now add data to matrix\n",
    "    grainedMatrix[,j] <- coarsedGrainedColData\n",
    "  }\n",
    "  return(grainedMatrix)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we define a simple multivariate probabilit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n",
      "[1] NaN\n"
     ]
    }
   ],
   "source": [
    "EvaluesV=c()\n",
    "  for(team in LISTOFTEAMS)\n",
    "  {\n",
    "    matchNYIDH<-subset(nhlDataDelta,nhlDataDelta$TEAM1==team)\n",
    "    matchDataNYIMatrix<-fetchDataColumnsAsMatrix(matchNYIDH)\n",
    "    grainedMatchDataNYIMatrix<-coarseGrainingGrainMatrix(matchDataNYIMatrix,centroidVectors)\n",
    "    Entropy<-getEntropy(McoarseGrained,centroidVectors,grainedMatchDataNYIMatrix)\n",
    "    #compute average entropy per game\n",
    "    Entropy<-Entropy/nrow(matchDataNYIMatrix)\n",
    "    print(Entropy)\n",
    "    EvaluesV <- c(EvaluesV,Entropy)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nhlDataMatrix=as.matrix(cbind(nhlDataDelta$WON,nhlDataDelta$SHOTS))#,nhlDataDelta$FACEOFF))#,nhlDataSum$SHOTS,nhlDataSum$FACEOFF))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using this matrix data a joint probability function is estimated, using the kernel estimator function kde from the package ks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.22324\n"
     ]
    }
   ],
   "source": [
    "estimateProbability <- function(inputVector)\n",
    "    {\n",
    "    Pks=kde(nhlDataMatrix,eval.points=inputVector)\n",
    "    return(Pks$estimate)\n",
    "    }\n",
    "#wins,shotsdifference,faceoffdifference,shotssum,faceoffsum\n",
    "iVec=c(1,5)#,5)#,50,40)\n",
    "print(estimateProbability(iVec))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This value is exceedingly small, reflecting the vastness of parameter space. We may now ask the question how probable it is to win, set that the home team has an advantage of more than 10 shots on goal compared to the guest team. To this end we sum up the probability value for all columns and the shot difference column from the maximal value down to 10 shots. I've written a function which calls the ks kde function and sums up probabilites for wins or losses, depending on the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getCondProbability <- function(getWin,PMINV,PMAXV)\n",
    "    {\n",
    "    #function calculating the conditional probability of win or loss given a parameter value for which to calculate the \n",
    "    # probability exceeds the input value\n",
    "    #input:\n",
    "    # getWin - 1 if want to now the probability of winning, else 0 (for loosing)\n",
    "    # parameterSlotIndex - index for the parameter for which to calculate the PDF\n",
    "    # parameterValue - value of the parameter which is assumed to be exceeded\n",
    "    P=0\n",
    "    p1=getWin\n",
    "    p2min=PMINV[2];p3min=PMINV[2];p4min=PMINV[3];p5min=PMINV[4];\n",
    "    p2max=PMAXV[2];p3max=PMAXV[2];p4max=PMAXV[3];p5max=PMAXV[4];\n",
    "    #always integer steps in our statistics\n",
    "        print(P)\n",
    "        for(p2 in p2min:p2max){\n",
    "          #  for(p3 in p3min:p3max){\n",
    "               # for(p4 in p4min:p4max){\n",
    "                   # for(p5 in p5min:p5max)\n",
    "                   #     {\n",
    "                            iVec=c(p1,p2)#,p3,p4,p5)\n",
    "                             #result=kde(nhlDataMatrix,eval.points=iVec)\n",
    "                             #P=P+result$estimate\n",
    "                            P=P+estimateProbability(iVec)\n",
    "                            print(P)\n",
    "                  #  }\n",
    "              #  }\n",
    "           # }\n",
    "        }\n",
    "    print(P)\n",
    "    message=paste(\"We have found a probability of\",P,\" %\")\n",
    "    print(message)\n",
    "    #return(message)\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's compute the probability of winning if a team has at least 10 shots more on goal than the opposing team. One could set an additional constraint just by constraining the global variables PMINV,PMAXV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0\n",
      "[1] 0.05386987\n",
      "[1] 0.1186611\n",
      "[1] 0.1974263\n",
      "[1] 0.2926446\n",
      "[1] 0.4054476\n",
      "[1] 0.5352806\n",
      "[1] 0.6800237\n",
      "[1] 0.8365848\n",
      "[1] 1.001971\n",
      "[1] 1.174484\n",
      "[1] 1.354304\n",
      "[1] 1.543027\n",
      "[1] 1.74248\n",
      "[1] 1.953546\n",
      "[1] 2.175523\n",
      "[1] 2.406115\n",
      "[1] 2.641935\n",
      "[1] 2.879234\n",
      "[1] 3.114531\n",
      "[1] 3.344922\n",
      "[1] 3.568163\n",
      "[1] 3.782657\n",
      "[1] 3.98738\n",
      "[1] 4.181667\n",
      "[1] 4.364881\n",
      "[1] 4.536141\n",
      "[1] 4.694265\n",
      "[1] 4.837949\n",
      "[1] 4.966011\n",
      "[1] 5.077635\n",
      "[1] 5.172696\n",
      "[1] 5.172696\n",
      "[1] \"We have found a probability of 5.17269625108125  %\"\n"
     ]
    }
   ],
   "source": [
    "#the third column goes from 10 shots to max value of 30 shots\n",
    "PMINV=c(0,-15,10,30,30)\n",
    "PMAXV=c(1,15,30,50,50)\n",
    "getCondProbability(1,PMINV,PMAXV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
